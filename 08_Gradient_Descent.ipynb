{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "assert dot([1, 2, 3], [4, 5, 6]) == 32  # 1 * 4 + 2 * 5 + 3 * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    '''Computes the sum of squared elements of v'''\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frameworks expecting callback functions of specific signatures \n",
    "# might be type hinted using Callable[[Arg1Type, Arg2Type], ReturnType].\n",
    "# https://docs.python.org/3/library/typing.html#callable\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float], x: float, h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x\n",
    "\n",
    "def deriverative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = range(-10, 11)\n",
    "list(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-20,\n",
       " -18,\n",
       " -16,\n",
       " -14,\n",
       " -12,\n",
       " -10,\n",
       " -8,\n",
       " -6,\n",
       " -4,\n",
       " -2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 16,\n",
       " 18,\n",
       " 20]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals = [deriverative(x) for x in xs]\n",
    "actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-19.998999999984335,\n",
       " -17.998999999988996,\n",
       " -15.999000000007868,\n",
       " -13.999000000005424,\n",
       " -11.99900000000298,\n",
       " -9.999000000004088,\n",
       " -7.998999999999867,\n",
       " -5.998999999999199,\n",
       " -3.9989999999994197,\n",
       " -1.998999999999973,\n",
       " 0.001,\n",
       " 2.0009999999996975,\n",
       " 4.000999999999699,\n",
       " 6.000999999999479,\n",
       " 8.0010000000037,\n",
       " 10.001000000002591,\n",
       " 12.001000000005035,\n",
       " 14.00100000000748,\n",
       " 16.000999999988608,\n",
       " 18.000999999983947,\n",
       " 20.000999999993496]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "plt.plot(xs, actuals, 'rx', label='Actual')\n",
    "plt.plot(xs, estimates, 'b+', label='Estimate')\n",
    "plt.legend(loc=9)\n",
    "plt.savefig('images/deriveratives.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The enumerate object yields pairs containing a count (from start, which\n",
    "# defaults to zero) and a value yielded by the iterable argument.\n",
    "\n",
    "def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                               v: Vector,\n",
    "                               i: int,\n",
    "                               h: float) -> float:\n",
    "    '''Returns the i=th partial difference quotient of f at v'''\n",
    "    for j, v_j in enumerate(v):\n",
    "        print(j, v_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 4\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "partial_difference_quotient(square, [1, 2, 3], 1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                               v: Vector,\n",
    "                               i: int,\n",
    "                               h: float) -> float:\n",
    "    '''Returns the i=th partial difference quotient of f at v'''\n",
    "    w = [v_j + (h if j == i else 0) # add h to just the ith element of v\n",
    "         for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                     v: Vector,\n",
    "                     h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i , h)\n",
    "           for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def add(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Adds corresponding elements\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be the same length\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "def subtract(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Subtracts corresponding elements\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be the same length\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "def scalar_multiply(c: float, v: Vector) -> Vector:\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def magnitude(v: Vector) -> float:\n",
    "    \"\"\"Returns the magnitude (or length) of v\"\"\"\n",
    "    return math.sqrt(sum_of_squares(v))   # math.sqrt is square root function\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> float:  # type: ignore\n",
    "    return magnitude(subtract(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.680660540649157, 1.5398814618718664, 2.9903369412349967]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random number in the range [a, b) or [a, b] depending on rounding.\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.4137844224210756, 7.7417647419816475, 4.368044840186407]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    '''Moves \"step_size\" in the \"gradient\" direction from \"v\"'''\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# pick a random starting point\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 [-2.577152493517081e-88, 1.4112270578603204e-87, 7.962400400764906e-88]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    # computer the gradient at v\n",
    "    grad = sum_of_squares_gradient(v)\n",
    "    #print(grad)\n",
    "    # take a negative gradient step (minimalization)\n",
    "    v = gradient_step(v, grad, -0.01)\n",
    "    #print(v)\n",
    "\n",
    "print(epoch, v)\n",
    "\n",
    "assert distance(v, [0,0,0]) < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-50, -995), (-49, -975), (-48, -955), (-47, -935), (-46, -915)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x ranges from -50 to 49, y is always 20 * x + 5\n",
    "inputs = [(x, 20*x+5) for x in range(-50, 50)]\n",
    "inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-5, -95),\n",
       " (-4, -75),\n",
       " (-3, -55),\n",
       " (-2, -35),\n",
       " (-1, -15),\n",
       " (0, 5),\n",
       " (1, 25),\n",
       " (2, 45),\n",
       " (3, 65),\n",
       " (4, 85)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[45:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(45, 905), (46, 925), (47, 945), (48, 965), (49, 985)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[95:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    # the prediction of the model\n",
    "    predicted = slope * x + intercept\n",
    "    # error = predicted - actual\n",
    "    error = (predicted - y)\n",
    "    # we will minimize squared error\n",
    "    squared_error = error ** 2\n",
    "    # using it's gradient\n",
    "    grad = [2 * error * x, 2 * error] # theta * error\n",
    "    #print(grad)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-168, -84]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_gradient(2, 45, [1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_mean(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "\n",
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.5721743094839724, 0.3551593134780371]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with a random values for slope and intercept\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1,1)]\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-34289.16973322326, 11.28249293644018]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputs = [(-50, -995), (-49, -975), (-48, -955), (-47, -935), (-46, -915)]\n",
    "# compute the mean of the gradients\n",
    "grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a step in that direction\n",
    "theta = gradient_step(theta, grad, -learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [10.846107929186424, 0.3669060623242528]\n",
      "100 [19.997719385576843, 1.2027756157064122]\n",
      "200 [19.998133052388674, 1.8915311058499984]\n",
      "300 [19.998471686687566, 2.45535741688973]\n",
      "400 [19.998748898165758, 2.916914694573862]\n",
      "500 [19.998975827935993, 3.294752898311352]\n",
      "600 [19.99916159629218, 3.604057322932017]\n",
      "700 [19.999313669253446, 3.8572588947783357]\n",
      "800 [19.999438158623025, 4.0645337698923045]\n",
      "900 [19.999540067621233, 4.234212313118681]\n",
      "1000 [19.999623491964627, 4.373113894970283]\n",
      "1100 [19.999691784472574, 4.486820961721426]\n",
      "1200 [19.999747689816893, 4.579903393590077]\n",
      "1300 [19.99979345483003, 4.65610216795072]\n",
      "1400 [19.999830918805127, 4.718479708991523]\n",
      "1500 [19.99986158741711, 4.769542966359436]\n",
      "1600 [19.999886693235656, 4.811344169316701]\n",
      "1700 [19.999907245262115, 4.845563305712269]\n",
      "1800 [19.9999240694812, 4.873575640593056]\n",
      "1900 [19.99993784205727, 4.8965069880239795]\n",
      "2000 [19.999949116509338, 4.915278957487997]\n",
      "2100 [19.999958345956955, 4.9306459933161095]\n",
      "2200 [19.999965901331073, 4.943225695759967]\n",
      "2300 [19.999972086281726, 4.953523642309074]\n",
      "2400 [19.99997714938171, 4.96195370681985]\n",
      "2500 [19.999981294116708, 4.968854693081242]\n",
      "2600 [19.99998468706337, 4.974503951318717]\n",
      "2700 [19.99998746458403, 4.97912852488325]\n",
      "2800 [19.999989738307068, 4.982914275110053]\n",
      "2900 [19.999991599613285, 4.986013351074514]\n",
      "3000 [19.99999312330846, 4.988550304454459]\n",
      "3100 [19.99999437062981, 4.990627095254623]\n",
      "3200 [19.99999539170711, 4.992327189573166]\n",
      "3300 [19.999996227577395, 4.993718914099158]\n",
      "3400 [19.99999691183424, 4.9948582021581345]\n",
      "3500 [19.99999747197789, 4.9957908416691]\n",
      "3600 [19.999997930520482, 4.99655431535827]\n",
      "3700 [19.999998305890813, 4.997179307187595]\n",
      "3800 [19.999998613175, 4.997690935541339]\n",
      "3900 [19.999998864722773, 4.998109762732478]\n",
      "4000 [19.99999907064382, 4.998452621400791]\n",
      "4100 [19.999999239214095, 4.998733291015671]\n",
      "4200 [19.99999937720844, 4.99896305167216]\n",
      "4300 [19.99999949017282, 4.999151137437321]\n",
      "4400 [19.999999582647277, 4.9993051074668085]\n",
      "4500 [19.999999658348354, 4.999431149806916]\n",
      "4600 [19.999999720318474, 4.9995343300917465]\n",
      "4700 [19.99999977104821, 4.9996187951308]\n",
      "4800 [19.999999812576384, 4.999687939568941]\n",
      "4900 [19.999999846572017, 4.9997445423170035]\n",
      "5000 [19.99999987440139, 4.999790878236049]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5001):\n",
    "    # compute the mean of the gradients\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # take a step in that direction\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch gradient descent\n",
    "from typing import TypeVar, List, Iterator\n",
    "\n",
    "T = TypeVar('T')  # this allows us to type \"generic\" functions\n",
    "\n",
    "def minibatches(dataset: List[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
    "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "\n",
    "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
    "\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [18.699906186758337, -0.6209055533513304]\n",
      "100 [20.13727990930935, 3.2661951680241144]\n",
      "200 [19.998045599074086, 4.498650141308391]\n",
      "300 [19.99979018923537, 4.894844820245764]\n",
      "400 [20.00131588562139, 4.979196326913925]\n",
      "500 [20.000786531808302, 4.996089672914272]\n",
      "600 [19.999989736570015, 4.999269340012567]\n",
      "700 [20.00000443995014, 4.999843396100309]\n",
      "800 [19.99998387055264, 4.999963422686699]\n",
      "900 [19.999999627276924, 4.9999923132652295]\n"
     ]
    }
   ],
   "source": [
    "# Minibatch gradient descent example\n",
    "    \n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [20.08554902655376, 0.7420526821561005]\n",
      "1 [20.08187797850584, 0.9247346577510066]\n",
      "2 [20.078365095070183, 1.0995788757622442]\n",
      "3 [20.07500293365887, 1.2669216186250032]\n",
      "4 [20.071785036901492, 1.4270847282964576]\n",
      "5 [20.068705197492037, 1.580376238276352]\n",
      "6 [20.065757475135896, 1.7270909655853186]\n",
      "7 [20.06293623025658, 1.8675110787830798]\n",
      "8 [20.06023602840389, 2.001906640689462]\n",
      "9 [20.057651681868016, 2.130536127367937]\n",
      "10 [20.055178203001066, 2.2536469250588107]\n",
      "11 [20.05281084041364, 2.37147580598555]\n",
      "12 [20.050545076191923, 2.4842493847612825]\n",
      "13 [20.048376487475686, 2.5921845527106844]\n",
      "14 [20.046300967999283, 2.6954888958171415]\n",
      "15 [20.044314467618506, 2.7943610938357737]\n",
      "16 [20.042413221182976, 2.8889913024713803]\n",
      "17 [20.04059353006525, 2.979561519194737]\n",
      "18 [20.03885192012222, 3.066245932826551]\n",
      "19 [20.037185039757738, 3.149211259295542]\n",
      "20 [20.035589641600158, 3.2286170608880598]\n",
      "21 [20.03406271659023, 3.3046160545050496]\n",
      "22 [20.032601287727008, 3.3773544051955553]\n",
      "23 [20.031202595945228, 3.446972007273781]\n",
      "24 [20.029863883223438, 3.513602752665438]\n",
      "25 [20.028582593098246, 3.577374787900947]\n",
      "26 [20.027356301249924, 3.6384107627060613]\n",
      "27 [20.026182619309328, 3.696828064826066]\n",
      "28 [20.02505927222374, 3.7527390446800655]\n",
      "29 [20.023984139149412, 3.8062512329216056]\n",
      "30 [20.02295512619014, 3.8574675469382]\n",
      "31 [20.02197027111184, 3.9064864884280834]\n",
      "32 [20.021027663948352, 3.953402332996571]\n",
      "33 [20.020125489870754, 3.998305311128286]\n",
      "34 [20.01926205262812, 4.041281782826031]\n",
      "35 [20.018435624037043, 4.082414402292076]\n",
      "36 [20.017644661424534, 4.121782277217072]\n",
      "37 [20.01688764619556, 4.159461122223822]\n",
      "38 [20.016163113993827, 4.195523403359226]\n",
      "39 [20.015469633339983, 4.230038476680437]\n",
      "40 [20.014805919859576, 4.2630727228939715]\n",
      "41 [20.014170713632595, 4.29468967598141]\n",
      "42 [20.01356272999317, 4.3249501431215345]\n",
      "43 [20.012980829764164, 4.353912322056626]\n",
      "44 [20.01242391004454, 4.381631914516635]\n",
      "45 [20.01189087311144, 4.408162232192392]\n",
      "46 [20.011380708247156, 4.433554299294959]\n",
      "47 [20.010892450483148, 4.45785695150085]\n",
      "48 [20.010425107255706, 4.48111692847187]\n",
      "49 [20.00997783150086, 4.503378964608876]\n",
      "50 [20.0095497424976, 4.524685875589079]\n",
      "51 [20.00914001916245, 4.545078639805687]\n",
      "52 [20.008747887094692, 4.564596477833143]\n",
      "53 [20.008372570560898, 4.583276927360702]\n",
      "54 [20.008013361040373, 4.601155915428473]\n",
      "55 [20.007669560116597, 4.618267827774416]\n",
      "56 [20.007340504855282, 4.634645574675262]\n",
      "57 [20.007025561728657, 4.650320654391516]\n",
      "58 [20.006724127893357, 4.665323213834622]\n",
      "59 [20.00643565536602, 4.679682107195208]\n",
      "60 [20.00615953336634, 4.693424950175594]\n",
      "61 [20.00589527045414, 4.706578173408762]\n",
      "62 [20.005642334933825, 4.719167073825926]\n",
      "63 [20.005400262680816, 4.7312158630338965]\n",
      "64 [20.005168570748676, 4.742747713938976]\n",
      "65 [20.004946812213003, 4.75378480488941]\n",
      "66 [20.00473458920077, 4.764348363254698]\n",
      "67 [20.0045314378482, 4.7744587050767695]\n",
      "68 [20.004337040688473, 4.784135275203462]\n",
      "69 [20.00415095238495, 4.793396684151437]\n",
      "70 [20.003972881007318, 4.802260743971009]\n",
      "71 [20.003802409453233, 4.810744502235982]\n",
      "72 [20.003639283362965, 4.8188642752026976]\n",
      "73 [20.00348314366188, 4.826635679558555]\n",
      "74 [20.00333370612175, 4.834073661453539]\n",
      "75 [20.003190670213908, 4.841192525823579]\n",
      "76 [20.003053779707574, 4.848005964008514]\n",
      "77 [20.00292276412638, 4.854527080144758]\n",
      "78 [20.002797361758358, 4.860768415817661]\n",
      "79 [20.002677356331372, 4.866741974855855]\n",
      "80 [20.002562491013848, 4.8724592460190275]\n",
      "81 [20.002452537544766, 4.877931224516476]\n",
      "82 [20.002347306601035, 4.883168434132641]\n",
      "83 [20.002246604588713, 4.888180947736677]\n",
      "84 [20.002150210353005, 4.892978405641278]\n",
      "85 [20.00205796356001, 4.897570034544469]\n",
      "86 [20.001969680531857, 4.9019646656295155]\n",
      "87 [20.001885165385907, 4.90617075054121]\n",
      "88 [20.00180427733718, 4.910196378267493]\n",
      "89 [20.001726865751362, 4.914049291257537]\n",
      "90 [20.00165277856184, 4.9177368998079505]\n",
      "91 [20.001581867849243, 4.921266296116155]\n",
      "92 [20.001514008868718, 4.924644268231594]\n",
      "93 [20.001449046481294, 4.927877312689459]\n",
      "94 [20.001386889628698, 4.930971647524543]\n",
      "95 [20.00132737557066, 4.933933223754511]\n",
      "96 [20.001270415160345, 4.936767736777832]\n",
      "97 [20.001215927321145, 4.939480638691669]\n",
      "98 [20.001163752721197, 4.942077147106964]\n",
      "99 [20.001113821450932, 4.944562255371513]\n"
     ]
    }
   ],
   "source": [
    "# Stochastic gradient descent example\n",
    "    \n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    #if epoch % 100 == 0:\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
